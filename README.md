# Arrhythmia Classification Using Machine Learning

## Overview

This study introduces a comprehensive framework for the automated analysis of electrocardiogram (ECG) data, addressing the critical need for robust and efficient methods in light of the vast volume of data generated by modern wearable technologies such as the Apple Watch. These devices have transformed healthcare by enabling continuous cardiovascular monitoring and facilitating the early detection of abnormalities.  
The project’s core objective is to develop and evaluate advanced classification techniques for arrhythmia detection. To achieve this, the framework integrates three key innovations:  

1. **Ensemble modelling** – combines the strengths of multiple classifiers to boost overall prediction accuracy.  
2. **Inclusion of interpretable “simple” models** – K-Nearest Neighbors (KNN), Logistic Regression, Naïve Bayes, and Decision Trees – to balance performance, transparency, and compute cost.  
3. **CNN-based image recognition** – converts 1-D ECG segments into 2-D images so a Convolutional Neural Network can capture subtle morphological features.

> **Keywords:** Machine Learning · ECG · Arrhythmia · Classification · XGBoost · KNN · Deep Learning · CNN  

---

## 1 · Introduction

Wearable devices such as the Apple Watch generate continuous ECG streams. Manual review is infeasible at this scale, motivating robust automated analysis. Prior work favours heavy deep-learning pipelines; however, in clinical practice *interpretability, latency, and power budgets* are equally critical. This study therefore benchmarks both lightweight and state-of-the-art models, highlighting the trade-offs that matter in real deployments.

---

## 2 · Dataset Overview: MIT-BIH Arrhythmia Database

The **MIT-BIH Arrhythmia Database** (48 half-hour, two-lead recordings; ≈110 k annotated beats) is used as the benchmark.

| Characteristic | Value |
|---------------|-------|
| Sampling frequency | 125 Hz (down-sampled from 360 Hz) |
| Total samples | 109 446 |
| Classes | `N` Normal · `S` Supraventricular premature · `V` PVC · `F` Fusion · `Q` Unclassifiable |

Pre-processing steps:  

1. Down-sampling to 125 Hz  
2. Beat-centred cropping  
3. Zero-padding to a fixed length (187 samples)  

Exploratory Data Analysis reveals a **severe class imbalance** (›70 k normal vs. ≪1 k minority beats). Rather than re-balancing for all pipelines, the study preserves the natural distribution for “simple” and “sophisticated” models; only the CNN branch trains on a balanced subset.

---

## 3 · Machine-Learning Models Employed

### 3.1 Simple Models
* **K-Nearest Neighbors (KNN)** – distance-weighted softmax voting  
* **Logistic Regression** – multinomial with softmax  
* **Decision Tree** – Gini split, depth tuned  
* **Naïve Bayes** – Gaussian assumption  

### 3.2 Sophisticated Models
* **XGBoost**  
* **LightGBM**  
* **Random Forest**  
* **Support Vector Classifier (SVC)** & **Linear SVC**

### 3.3 Deep-Learning Model
* **2-D Convolutional Neural Network** applied to image-converted ECG beats (100 × 100 px PNG)

---

## 4 · Methodology and Experimental Setup

1. **Data split** – 70 % train · 30 % test  
2. **PCA** – top 30 components for tabular pipelines  
3. **Training & hyper-tuning** – grid / random search as appropriate  
4. **Evaluation metrics** – Accuracy, Precision, Recall, F₁, ROC-AUC  
5. **CNN workflow** – class down-sampling → signal-to-image → custom 7-layer CNN (ReLU, MaxPool, Dropout, Softmax)

---

## 5 · Results and Performance Analysis

### 5.1 Simple Models (full imbalanced data)

| Model | Accuracy | Precision | Recall | F₁ |
|-------|---------:|----------:|-------:|---:|
| **KNN** | 0.9711 | 0.929 | 0.854 | 0.876 |
| Decision Tree | 0.9547 | 0.813 | 0.800 | 0.806 |
| Logistic Regression | 0.9162 | 0.803 | 0.776 | 0.793 |
| Naïve Bayes | *poor* | – | – | – |

### 5.2 Sophisticated Models

| Model | Accuracy | Precision | Recall | F₁ |
|-------|---------:|----------:|-------:|---:|
| **XGBoost** | **0.9813** | 0.960 | 0.855 | **0.900** |
| LightGBM | 0.9801 | 0.946 | 0.844 | 0.887 |
| Random Forest | 0.9754 | **0.970** | 0.813 | 0.877 |
| SVC | 0.9670 | 0.944 | 0.757 | 0.827 |
| Linear SVC | 0.9041 | 0.826 | 0.451 | 0.490 |

### 5.3 CNN (balanced subset)

| Class | Precision | Recall | F₁ |
|------:|----------:|-------:|---:|
| 0 | 0.69 | 0.84 | 0.76 |
| 1 | 0.85 | 0.71 | 0.77 |
| 2 | 0.85 | 0.79 | 0.82 |
| 3 | 0.90 | 0.91 | 0.90 |
| 4 | **0.96** | **0.96** | **0.96** |
| **Macro Avg** | **0.85** | **0.84** | **0.85** |
| **Overall Acc** | **0.84** | — | — |

---

## 6 · Limitations

* **XGBoost** – memory- and compute-heavy; unsuitable for always-on, on-device inference.  
* **KNN** – stores entire training set; inference latency scales poorly with data size.  
* Minority-class recall remains challenging for all tabular models without explicit balancing.

---

## 7 · Future Scope

1. **Model compression** – pruning, quantization, TinyML (TensorFlow Lite Micro, Edge Impulse) to fit < 100 KB binaries.  
2. **Edge / cloud hybrid** – off-load heavy analytics while retaining low-latency alerts on-device.  
3. **End-to-end pipeline** – acquisition → pre-processing → inference → alerting.  
4. **Clinical validation** – trials on diverse cohorts; minimize false alarms.

---

## 8 · Conclusion

LightGBM offers near-state-of-the-art accuracy **with lower resource cost** than XGBoost, whereas a tuned **Decision Tree** delivers a strong F₁ (~ 0.955) in an extremely lightweight and *explainable* package – a compelling option for battery-constrained wearables where transparency drives trust.

---

## 9 · Code and Data Access

A QR code in the original appendix links to:  

* `arrhythmia_classification.py` – full training script  
* `mitbih_train.csv`, `mitbih_test.csv` – pre-processed datasets  
* `cnn_saved_model.h5` – trained CNN weights  

---

## 10 · Bibliography

A full reference list is provided in `docs/bibliography.bib`.

