# -*- coding: utf-8 -*-
"""rt cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ss4noPv-o57VNxGZtiMAZnvLSvLAYnEt
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.utils import resample
import os

# Load data
def load_data():
    train = pd.read_csv('/content/drive/My Drive/rt/mitbih_train.csv', header=None)
    test = pd.read_csv('/content/drive/My Drive/rt/mitbih_test.csv', header=None)
    return train, test

train, test = load_data()

# Separate features and labels
X_train = train.iloc[:, :-1]
y_train = train.iloc[:, -1]
X_test = test.iloc[:, :-1]
y_test = test.iloc[:, -1]

# Downsample function
def downsample(X, y, n_samples=200):
    data = pd.concat([X, y], axis=1)
    downsampled = []

    for label in y.unique():
        class_data = data[data.iloc[:, -1] == label]
        if len(class_data) >= n_samples:
            downsampled.append(resample(class_data, replace=False, n_samples=n_samples, random_state=42))
        else:
            print(f"Warning: Class {label} has only {len(class_data)} samples. Using all available.")
            downsampled.append(class_data)

    return pd.concat(downsampled)

# Apply downsampling: 300 per class for train, 100 per class for test
downsampled_train = downsample(X_train, y_train, n_samples=400)
downsampled_test = downsample(X_test, y_test, n_samples=150)

# Separate features and labels after downsampling
X_train = downsampled_train.iloc[:, :-1].values
y_train = downsampled_train.iloc[:, -1].values
X_test = downsampled_test.iloc[:, :-1].values
y_test = downsampled_test.iloc[:, -1].values

# Print final shapes
print(f"Downsampled Train Shape: {X_train.shape}, {y_train.shape}")
print(f"Downsampled Test Shape: {X_test.shape}, {y_test.shape}")

# Optional: Print class distributions
print("Class distribution in downsampled train:", pd.Series(y_train).value_counts().sort_index())
print("Class distribution in downsampled test:", pd.Series(y_test).value_counts().sort_index())

import matplotlib.pyplot as plt
import numpy as np

# Create directories to save images
os.makedirs('train_images', exist_ok=True)
os.makedirs('test_images', exist_ok=True)

# Function to save images
def generate_images(X, y, folder):
    for idx, signal in enumerate(X):
        plt.figure(figsize=(3, 3))
        plt.plot(signal, linewidth=0.5)
        plt.axis('off')
        plt.savefig(f'{folder}/{y[idx]}_{idx}.png', bbox_inches='tight', pad_inches=0)
        plt.close()

# Generate images for train and test
generate_images(X_train, y_train, 'train_images')
generate_images(X_test, y_test, 'test_images')

import tensorflow as tf
from tensorflow.keras import layers, models

# Define 2D CNN model
def create_cnn_model(input_shape, num_classes):
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Example input shape for grayscale images
input_shape = (100, 100, 1)  # Adjust this based on image size
num_classes = len(np.unique(y_train))
model = create_cnn_model(input_shape, num_classes)

import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models, optimizers
from sklearn.model_selection import train_test_split

# If you're using Google Colab and storing images in Google Drive:
# from google.colab import drive
# drive.mount('/content/drive')

# Adjust paths as needed
IMAGES_FOLDER_TRAIN = "/content/train_images"  # Example path for training images
IMAGES_FOLDER_TEST  = "/content/test_images"   # Example path for testing images

def build_dataframe_from_directory(directory):
    """
    Scans the given directory for images, parses the class label from each filename,
    and returns a pandas DataFrame with columns: ['filename', 'class'].
    Example filename: '0.0_10.png' -> class label = '0.0'
    """
    data = []
    for file_name in os.listdir(directory):
        if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
            # Parse label from filename
            # For example, "0.0_10.png" -> label "0.0"
            label = file_name.split("_")[0]  # everything before the first underscore
            data.append([file_name, label])

    df = pd.DataFrame(data, columns=['filename', 'class'])
    return df

# Build DataFrame for train and test
df_train = build_dataframe_from_directory(IMAGES_FOLDER_TRAIN)
df_test = build_dataframe_from_directory(IMAGES_FOLDER_TEST)

print("Train DataFrame Head:")
print(df_train.head())
print("\nTest DataFrame Head:")
print(df_test.head())

import os
import pandas as pd

def build_dataframe(directory):
    """
    Scans the directory for image files and extracts the class label from the filename.
    Assumes filenames like "0.0_0.png" where the label is the part before the underscore.
    """
    data = []
    for fname in os.listdir(directory):
        if fname.lower().endswith(('.png', '.jpg', '.jpeg')):
            # Extract label: For example, "0.0_0.png" --> label "0.0"
            label = fname.split('_')[0]
            data.append({'filename': fname, 'class': label})
    df = pd.DataFrame(data)
    return df

# Define your folder paths (adjust if needed)
train_folder = 'train_images'
test_folder  = 'test_images'

# Build DataFrames
df_train = build_dataframe(train_folder)
df_test = build_dataframe(test_folder)

print("Train DataFrame:")
print(df_train.head())
print("\nTest DataFrame:")
print(df_test.head())

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Create a rescaling ImageDataGenerator (no validation split since we have separate test data)
datagen = ImageDataGenerator(rescale=1./255)

# Generator for the training set
train_generator = datagen.flow_from_dataframe(
    dataframe=df_train,
    directory=train_folder,       # Folder containing training images
    x_col="filename",
    y_col="class",
    target_size=(100, 100),       # Adjust target size as needed
    color_mode='grayscale',       # Use 'rgb' if your images are in color
    class_mode='sparse',          # Labels are in sparse format
    batch_size=32,
    shuffle=True
)

# Generator for the test set
test_generator = datagen.flow_from_dataframe(
    dataframe=df_test,
    directory=test_folder,        # Folder containing test images
    x_col="filename",
    y_col="class",
    target_size=(100, 100),
    color_mode='grayscale',
    class_mode='sparse',
    batch_size=32,
    shuffle=False
)

print(f"Training images found: {train_generator.samples}")
print(f"Test images found: {test_generator.samples}")

from tensorflow.keras import layers, models, optimizers

def create_cnn_model(input_shape=(100, 100, 1), num_classes=5):
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Determine the number of classes from the training generator
num_classes = len(train_generator.class_indices)
model = create_cnn_model(input_shape=(100, 100, 1), num_classes=num_classes)
model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Define callbacks for early stopping and model checkpointing
checkpoint_path = 'best_model.h5'
callbacks_list = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ModelCheckpoint(checkpoint_path, save_best_only=True, verbose=1)
]

history = model.fit(
    train_generator,
    epochs=10,  # Adjust the number of epochs as needed
    callbacks=callbacks_list
)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.title("Training Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.title("Training Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.show()

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(test_generator)
print(f"Test Accuracy: {test_acc:.2f}")

# Predict on the test set
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_generator.classes

# Print classification report
print("Classification Report:")
print(classification_report(true_classes, predicted_classes))

# Plot confusion matrix
cm = confusion_matrix(true_classes, predicted_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

model.save('final_model.h5')

!zip -r train_images.zip train_images
!zip -r test_images.zip test_images

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Define callbacks (optional but useful)
checkpoint_path = 'best_model.h5'
callbacks_list = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ModelCheckpoint(checkpoint_path, save_best_only=True, verbose=1)
]

# Train the model
history = model.fit(
    train_generator,
    epochs=10,  # Adjust epochs as needed
    callbacks=callbacks_list
)

# Evaluate on training data
train_loss, train_acc = model.evaluate(train_generator)
print(f"Training Accuracy: {train_acc:.2f}")

# Evaluate on test data
test_loss, test_acc = model.evaluate(test_generator)
print(f"Test Accuracy: {test_acc:.2f}")

# Optionally, make predictions on the test set and display a classification report
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predict on test set
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_generator.classes

print("Classification Report:")
print(classification_report(true_classes, predicted_classes))

# Plot confusion matrix
cm = confusion_matrix(true_classes, predicted_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

